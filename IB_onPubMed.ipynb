{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline calculation with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class PubMedGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(data):\n",
    "    model = PubMedGNN(data.x.shape[1], 64, data.y.max().item() + 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        preds = logits[data.test_mask].argmax(dim=1)\n",
    "        balanced_acc = balanced_accuracy_score(data.y[data.test_mask].cpu(), preds.cpu())\n",
    "\n",
    "    return balanced_acc\n",
    "\n",
    "# Load PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed', transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "balanced_acc = train_and_evaluate(data)\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gib training on 2 layer architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5026302337646484, Balanced Accuracy: 0.3749\n",
      "Epoch 10, Loss: 1.1825270652770996, Balanced Accuracy: 0.3726\n",
      "Epoch 20, Loss: 0.9962648749351501, Balanced Accuracy: 0.5701\n",
      "Epoch 30, Loss: 0.8259830474853516, Balanced Accuracy: 0.6610\n",
      "Epoch 40, Loss: 0.6544902324676514, Balanced Accuracy: 0.8120\n",
      "Epoch 50, Loss: 0.5252289175987244, Balanced Accuracy: 0.8356\n",
      "Epoch 60, Loss: 0.4669717252254486, Balanced Accuracy: 0.8439\n",
      "Epoch 70, Loss: 0.44023898243904114, Balanced Accuracy: 0.8525\n",
      "Epoch 80, Loss: 0.42363640666007996, Balanced Accuracy: 0.8598\n",
      "Epoch 90, Loss: 0.4130460321903229, Balanced Accuracy: 0.8652\n",
      "Epoch 100, Loss: 0.4052874743938446, Balanced Accuracy: 0.8692\n",
      "Epoch 110, Loss: 0.39930373430252075, Balanced Accuracy: 0.8716\n",
      "Epoch 120, Loss: 0.3943099081516266, Balanced Accuracy: 0.8740\n",
      "Epoch 130, Loss: 0.3900507688522339, Balanced Accuracy: 0.8765\n",
      "Epoch 140, Loss: 0.3863217234611511, Balanced Accuracy: 0.8783\n",
      "Epoch 150, Loss: 0.38299494981765747, Balanced Accuracy: 0.8801\n",
      "Epoch 160, Loss: 0.3799697458744049, Balanced Accuracy: 0.8813\n",
      "Epoch 170, Loss: 0.3771507143974304, Balanced Accuracy: 0.8825\n",
      "Epoch 180, Loss: 0.3744357228279114, Balanced Accuracy: 0.8842\n",
      "Epoch 190, Loss: 0.37176141142845154, Balanced Accuracy: 0.8847\n",
      "Final Balanced Accuracy: 0.8865\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "\n",
    "class GIBModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, beta=0.1):\n",
    "        super(GIBModel, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        self.prior_dist = torch.distributions.Normal(0, 1)  # Gaussian prior for Z\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z1 = F.relu(self.conv1(x, edge_index))  # First GCN layer\n",
    "        z2 = self.conv2(z1, edge_index)  # Second GCN layer\n",
    "        return z1, z2\n",
    "\n",
    "    def compute_gib_loss(self, z, y, prior_dist, target_dist, edge_index):\n",
    "        # Cross-entropy for I(Y; Z_X^(L)) - Task relevance\n",
    "        prediction_loss = F.cross_entropy(z, y)\n",
    "        \n",
    "        # KL divergence for I(D; Z_X^(L)) - Compression term\n",
    "        kl_div = torch.distributions.kl_divergence(target_dist, prior_dist).mean()\n",
    "        \n",
    "        # Combine losses\n",
    "        gib_loss = prediction_loss + self.beta * kl_div\n",
    "        return gib_loss\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z1, z2 = model(data.x, data.edge_index)  # Forward pass\n",
    "        predictions = z2.argmax(dim=1)  # Get predicted class labels\n",
    "        true_labels = data.y.cpu().numpy()  # Ground truth labels\n",
    "        pred_labels = predictions.cpu().numpy()  # Predicted labels\n",
    "\n",
    "        # Calculate Balanced Accuracy\n",
    "        balanced_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "        return balanced_acc\n",
    "\n",
    "def train_gib_model(data, input_dim, hidden_dim, output_dim, beta=0.1, epochs=200, lr=0.01):\n",
    "    model = GIBModel(input_dim, hidden_dim, output_dim, beta=beta).to(data.x.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z1, z2 = model(data.x, data.edge_index)\n",
    "        prior_dist = model.prior_dist\n",
    "        target_dist = torch.distributions.Normal(z2.mean(), z2.std())  # Variational posterior\n",
    "\n",
    "        # Compute GIB loss\n",
    "        loss = model.compute_gib_loss(z2, data.y, prior_dist, target_dist, data.edge_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            balanced_acc = evaluate_model(model, data)\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}, Balanced Accuracy: {balanced_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load data and run the training function\n",
    "data = dataset[0]\n",
    "input_dim = dataset.num_node_features\n",
    "hidden_dim = 64\n",
    "output_dim = dataset.num_classes\n",
    "beta = 0.1  # Ideal beta value can be tuned\n",
    "trained_model = train_gib_model(data, input_dim, hidden_dim, output_dim, beta=beta)\n",
    "\n",
    "# Final Evaluation\n",
    "final_balanced_accuracy = evaluate_model(trained_model, data)\n",
    "print(f'Final Balanced Accuracy: {final_balanced_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understanding the change with the tradeoff factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value evaluatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
